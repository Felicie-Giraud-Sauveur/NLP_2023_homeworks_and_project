{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0Iyg6btLW9M"
   },
   "source": [
    "#  Assignment 2 - Transfer Learning and Data Augmentation üí¨\n",
    "\n",
    "Welcome to the **second assignment** for the **CS-552: Modern NLP course**!\n",
    "\n",
    "> - üòÄ Name: **< F√©licie Giraud-Sauveur >**\n",
    "> - ‚úâÔ∏è Email: **< felicie.giraud-sauveur >@epfl.ch**\n",
    "> - ü™™ SCIPER: **284220**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XjnQhbFIJUu"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;\">\n",
    "\n",
    "## **Assignment Description**\n",
    "- In the first part of this assignment, you will need to implement training (fine-tuning) and evaluation of a pre-trained language model ([DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) ), on natural language inference (NLI) task for recognizing textual entailment (RTE).\n",
    "\n",
    "- Following the first finetuning task, you will need to identify the shortcut (i.e. some salient or toxic features) that the model learnt for the specific task. \n",
    "\n",
    "- For part-3, you are supposed to annotate 100 randomly assigned test datapoints as ground-truth labels. Additionally, the cross annotation should be conducted by another one or two annotators, and you will learn about how to calculate the agreement statistics as a significant characteristic reflecting the quality of a collected dataset.\n",
    "\n",
    "- For part-4, since the human annotation is quite time- and effort-consuming, there are plenty of ways to get silver-labels from automatic labeling to augment the dataset scale. We provide the reference to some simple methods (EDA and Back Translation) but you are encouraged to explore other advanced mechanisms. You will evaluate the improvement of your model performance by using your data augmentation method.\n",
    "\n",
    "For each part, you will need to complete the code in the corresponding `.py` files (`nli.py` for Part-1, `shortcut.py` for Part-2, `eda.py` for Part-4). You will be provided with the function descriptions and detailed instructions about the code snippet you need to write.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "- **[PART 1: Model Finetuning for NLI](#1)**\n",
    "    - [1.1 Data Processing](#11)\n",
    "    - [1.2 Model Training and Evaluation](#12)\n",
    "- **[PART 2: Identify Model Shortcut](#2)**\n",
    "    - [2.1 Word-Pair Pattern Extraction](#21)\n",
    "    - [2.2 Distill Potentially Useful Patterns](#22)\n",
    "    - [2.3 Case Study](#23)\n",
    "- **[PART 3: Annotate New Data](#3)**\n",
    "    - [3.1 Write an Annotation Guideline](#31)\n",
    "    - [3.2 Annotate Your 100 Datapoints with Partner(s)](#32)\n",
    "    - [3.3 Agreement Measure](#33)\n",
    "    - [3.4 Robustness Check](#34)\n",
    "- **[PART 4: Data Augmentation](#4)**\n",
    "    \n",
    "### Deliverables\n",
    "\n",
    "- ‚úÖ This jupyter notebook\n",
    "- ‚úÖ `nli.py` file\n",
    "- ‚úÖ `shortcut.py` file\n",
    "- ‚úÖ Finetuned DistilBERT models for NLI task (Part 1 and Part 4)\n",
    "- ‚úÖ Annotated and cross-annotated data files (Part 3)\n",
    "- ‚úÖ New dataset from data augmentation (Part 4)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lluaZwaS-0v9"
   },
   "source": [
    "### Google Colab Setup\n",
    "If you are using Google Colab notebook for this assignment, you will need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the popped window, sign in to your Google account. (The same account you used to store this notebook!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VfVHqiSvK1aB"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opCteyIv-_kS"
   },
   "source": [
    "Now first click the 4th left-side bar (named Files), then click the 2nd bar popped under Files column (named Refresh), under \"/drive/MyDrive/\" find the Assignment 2 folder that you uploaded to your Google Drive, copy its path and fill it in below. If everything is working correctly, then running the folowing cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "['Assignment2.ipynb', 'requirements.txt', 'runs', 'predictions', 'nli_data', 'testA2.py', 'nli.py', 'shortcut.py']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "agEgK0kdrUdT"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "# TODO: Fill in the path where you download the Assignment folder into\n",
    "#ROOT_PATH = \"/content/drive/...\" # Replace with your directory to A2 folder\n",
    "#print(os.listdir(ROOT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5mABwvHy5-e"
   },
   "source": [
    "Before we start, we also need to run some boilerplate code to set up our environment, same as previous assignments. You'll need to rerun this setup code each time you start the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZhJT7Fo4_D1f"
   },
   "outputs": [],
   "source": [
    "#requirements = ROOT_PATH + \"/requirements.txt\"\n",
    "#!pip install -r {requirements}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUw9ycDa21dl"
   },
   "source": [
    "\n",
    "Run this cell to load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3PVAoLPQ_I7c"
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mA1Qk-_K_LRm"
   },
   "outputs": [],
   "source": [
    "#from copy import deepcopy\n",
    "#import numpy as np \n",
    "#from tqdm import tqdm\n",
    "#import jsonlines\n",
    "#import sys\n",
    "#import time\n",
    "#import random\n",
    "\n",
    "#import torch\n",
    "#import torch.utils.data\n",
    "#from torch import nn, optim\n",
    "#from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#from transformers import AdamW, get_constant_schedule_with_warmup\n",
    "#from transformers import DistilBertTokenizer, DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csa48DhDr0td"
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello A2!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "U10S-b9BrNxj"
   },
   "outputs": [],
   "source": [
    "#sys.path.append(ROOT_PATH)\n",
    "\n",
    "#from testA2 import hello_A2\n",
    "#hello_A2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnWpzntscWUE"
   },
   "source": [
    "Note that if CUDA is not enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rqb9cwkNIEHr"
   },
   "outputs": [],
   "source": [
    "#if torch.cuda.is_available():\n",
    "    #print('Good to go!')\n",
    "#else:\n",
    "    #print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MSpYuMcyHfl"
   },
   "source": [
    "### Local Setup\n",
    "If you skip Google Colab setup, you still need to fill in the path where you download the Assignment folder, and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QQHs7vHhydij"
   },
   "outputs": [],
   "source": [
    "#ROOT_PATH = \"...\" # Replace with your directory to A2 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IKeGSDpAyigE"
   },
   "outputs": [],
   "source": [
    "#requirements = \"requirements.txt\"  #ROOT_PATH + \"/requirements.txt\"\n",
    "#!pip install -r {requirements}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "C6b-Enimyywz"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q6dgLZ9kyqpO"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import jsonlines\n",
    "import sys\n",
    "import time, os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_constant_schedule_with_warmup\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHhgkhaH-IUl"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## **PART 1: Finetuning DistilBERT for NLI**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD2YPuqeIYBN"
   },
   "source": [
    "### **What is the NLI task?üßê**\n",
    "> Given a pair of sentences, denoted as a \"premise\" sentence and a \"hypothesis\" sentence, NLI (or RTE) aims to determine their logical relationship, i.e. whether they are logically follow (entailment), unfollow (contradiction) or are undetermined (neutral) to each other.\n",
    "\n",
    "> Defined as a machine learning task, NLI can be considered as a 3-classes (entailment, contradiction, or neutral) classification task, with a sentence-pair input (\"hypothesis\" and ‚Äúpremise‚Äù).\n",
    "\n",
    "> **You can run the following cell to have the first glance at your data**. Each data sample is a python dictionary, which consists of following components:\n",
    "- premise sentence (*'premise'*), \n",
    "- hypothesis sentence (*'hypothesis'*) \n",
    "- domain (*'domain'*): describing the topic of premise and hypothesis sentences (e.g., government regulations, telephone talks, etc.)\n",
    "- label (*'label'*): indicating the logical relation between premise and hypothesis (i.e., entailment, contradiction, or neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p-ODgcNUqYtm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': 'The new rights are nice enough', 'hypothesis': 'Everyone really likes the newest benefits ', 'domain': 'slate', 'label': 'neutral'}\n",
      "{'premise': 'This site includes a list of all award winners and a searchable database of Government Executive articles.', 'hypothesis': 'The Government Executive articles housed on the website are not able to be searched.', 'domain': 'government', 'label': 'contradiction'}\n",
      "{'premise': \"uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him\", 'hypothesis': 'I like him for the most part, but would still enjoy seeing someone beat him.', 'domain': 'telephone', 'label': 'entailment'}\n"
     ]
    }
   ],
   "source": [
    "# If you use Google Colab, then data_dir = 'GOOGLE_DRIVE_PATH/nli_data'\n",
    "data_dir = 'nli_data'  #ROOT_PATH+'/nli_data'\n",
    "data_dev_path = os.path.join(data_dir, 'dev_in_domain.jsonl')\n",
    "with jsonlines.open(data_dev_path, \"r\") as reader:\n",
    "    for sid, sample in enumerate(reader.iter()):\n",
    "        print(sample)\n",
    "        if sid == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FF-dRWc7MZlL"
   },
   "outputs": [],
   "source": [
    "# Enter your Sciper number\n",
    "SCIPER = '284220'\n",
    "seed = int(SCIPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UPdzLSi4ZVt",
    "outputId": "9aac2886-168e-4182-bc3c-aa7589c72c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your random seed is:  284220\n"
     ]
    }
   ],
   "source": [
    "print('Your random seed is: ', seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GnX8VC4C0sHW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We use the following pretrained tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCETOFT2dB4u"
   },
   "source": [
    "### **1.1 Dataset Processing**\n",
    "Our first step is to load datasets for NLI task by constructing a Pytorch Dataset. Specifically, we will need to implement tokenization and padding with a HuggingFace pre-trained tokenizer.\n",
    "\n",
    "**Complete `NLIDataset` class following the instructions in `nli.py`, and test by running the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_5Sya9W5BTDl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9815it [00:07, 1378.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLIDataset test correct ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\felic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nli import NLIDataset\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "dataset = NLIDataset(\"nli_data/dev_in_domain.jsonl\", tokenizer)  #NLIDataset(ROOT_PATH+\"/nli_data/dev_in_domain.jsonl\", tokenizer)\n",
    "\n",
    "from testA2 import test_NLIDataset\n",
    "\n",
    "test_NLIDataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0weQpG6_3vO"
   },
   "source": [
    "### **1.2 Model Training and Evaluation**\n",
    "Next, we will implement the training and evaluation process to finetune the model. For model training, you will need to calculate the loss and update the model weights by update the optimizer. Additionally, we add a learning rate schedular to adopt an adaptive learning rate during the whole training process. \n",
    "\n",
    "For evaluation, you will need to compute accuracy and F1 scores to assess the model performance. \n",
    "\n",
    "**Complete the `compute_metric()`, `train()` and `evaluate()` functions following the instructions in the `nli.py` file, you can test compute_metric() by running the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6w7Leraw4tIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_metric test correct ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "from nli import compute_metrics, train, evaluate\n",
    "\n",
    "from testA2 import test_compute_metrics\n",
    "test_compute_metrics(compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvCUS748_3vS"
   },
   "source": [
    "#### **Start Training and Validation!**\n",
    "\n",
    "Try the following different hyperparameter settings, compare and discuss the results. (Other hyperparameters should not be changed.)\n",
    "\n",
    "> A. learning_rate 2e-5\n",
    "\n",
    "> B. learning_rate 5e-5\n",
    "\n",
    "**Note:** *Each training will take about 1 hour using a GPU, please keep your computer and notebook active during the training.*\n",
    "\n",
    "**Questions: Which learning rate is better? Explain your answers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zn66mMOj_3vS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98176it [01:13, 1333.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9815it [00:07, 1382.74it/s]\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model.to(device)\n",
    "\n",
    "train_dataset = NLIDataset(\"nli_data/train.jsonl\", tokenizer) #NLIDataset(ROOT_PATH+\"/nli_data/train.jsonl\", tokenizer)\n",
    "dev_dataset = NLIDataset(\"nli_data/dev_in_domain.jsonl\", tokenizer) # NLIDataset(ROOT_PATH+\"/nli_data/dev_in_domain.jsonl\", tokenizer)\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "warmup_percent = 0.3\n",
    "model_save_root = 'runs/' #ROOT_PATH+'/runs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "an7lzsAywGxE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felic\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:57<00:00,  6.41it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:26<00:00, 23.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 0.716 | Validation Loss: 0.591\n",
      "Epoch 0 NLI Validation:\n",
      "Accuracy: 76.07% | F1: (78.79%, 72.24%, 77.18%) | Macro-F1: 76.07%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:44<00:00,  6.49it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:25<00:00, 23.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.465 | Validation Loss: 0.600\n",
      "Epoch 1 NLI Validation:\n",
      "Accuracy: 77.30% | F1: (80.44%, 72.62%, 78.49%) | Macro-F1: 77.18%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:46<00:00,  6.48it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:25<00:00, 23.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training Loss: 0.244 | Validation Loss: 0.758\n",
      "Epoch 2 NLI Validation:\n",
      "Accuracy: 76.95% | F1: (80.33%, 72.29%, 77.79%) | Macro-F1: 76.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:45<00:00,  6.49it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:25<00:00, 23.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training Loss: 0.147 | Validation Loss: 1.024\n",
      "Epoch 3 NLI Validation:\n",
      "Accuracy: 76.96% | F1: (80.38%, 72.34%, 77.62%) | Macro-F1: 76.78%\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "learning_rate = 2e-5 # play around with this hyperparameter\n",
    "\n",
    "train(train_dataset, dev_dataset, model, device, batch_size, epochs,\n",
    "      learning_rate, warmup_percent, max_grad_norm, model_save_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:49<00:00,  6.46it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:25<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 0.329 | Validation Loss: 0.708\n",
      "Epoch 0 NLI Validation:\n",
      "Accuracy: 76.11% | F1: (78.71%, 72.68%, 76.72%) | Macro-F1: 76.04%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:38<00:00,  6.54it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:25<00:00, 24.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.222 | Validation Loss: 0.862\n",
      "Epoch 1 NLI Validation:\n",
      "Accuracy: 75.57% | F1: (79.03%, 69.97%, 77.13%) | Macro-F1: 75.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:41<00:00,  6.52it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:25<00:00, 23.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training Loss: 0.192 | Validation Loss: 1.027\n",
      "Epoch 2 NLI Validation:\n",
      "Accuracy: 74.80% | F1: (77.21%, 70.42%, 76.66%) | Macro-F1: 74.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:39<00:00,  6.53it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:25<00:00, 23.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training Loss: 0.170 | Validation Loss: 1.166\n",
      "Epoch 3 NLI Validation:\n",
      "Accuracy: 75.32% | F1: (78.15%, 71.23%, 76.27%) | Macro-F1: 75.22%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-5 # play around with this hyperparameter\n",
    "\n",
    "train(train_dataset, dev_dataset, model, device, batch_size, epochs,\n",
    "      learning_rate, warmup_percent, max_grad_norm, model_save_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#6699FF; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#6699FF\"> Conclusion: </font> </b>\n",
    "<ul style=\"list-style-type:circle\"> \n",
    "<li> <b> The learning rate 2e-5 seems better than the learning rate 5e-5. Indeed, when we look at the validation scores, the 2e-5 learning rate has a better accuracy, better F1 scores for each category and a better Macro-F1. Thus, the model with the training with the 2e-5 learning rate seems to generalize better.  </b></li>\n",
    "<li> <b> When we look at the training, we see that the training with a learning rate of 2e-5 has a higher training loss for the first epochs but that this loss decreases faster than for a training with a learning rate of 5e-5.  For the validation loss during the training, we see that it is lower for the training with a learning rate of 2e-5, again this model seems to generalize better. </b></li> \n",
    "</ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGuzGJCB_3vT"
   },
   "source": [
    "### **Fine-Grained Validation**\n",
    "\n",
    "Use the model checkpoint saved under the first hyperparameter setting (learning_rate 2e-5) in 1.4, check the model performance on each domain subsets of the validation set, report the validation loss, accuracy, F1 scores and Macro-F1 on each domain, compare and discuss the results.\n",
    "\n",
    "**Questions: On which domain does the model perform the best? the worst? Give some possible explanations of why the model's best-performed domain is easier, and why the model's worst-performed domain is more challenging. Use some examples to support your explanations.**\n",
    "\n",
    "**Note:** To find examples for supporting your discussion, save the model prediction results on each domain under the './predictions/' folder, by specifying the *result_save_file* of the *evaluate* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YCWWJjTP_3vT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9815it [00:00, 208790.90it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "warmup_percent = 0.3\n",
    "checkpoint = 'runs/lr{}-warmup{}'.format(learning_rate, warmup_percent) #ROOT_PATH+'/runs/lr{}-warmup{}'.format(learning_rate, warmup_percent)\n",
    "\n",
    "# Split the validation sets into subsets with different domains and save the subsets under './nli_data/'\n",
    "\n",
    "    # Split validation set in domains\n",
    "split_domains = defaultdict(list)\n",
    "with jsonlines.open(\"nli_data/dev_in_domain.jsonl\", \"r\") as reader:\n",
    "    for sample in tqdm(reader.iter()):\n",
    "        split_domains[sample[\"domain\"]].append(sample) \n",
    "\n",
    "    # Save each domain in a json file\n",
    "for domain in split_domains.keys():\n",
    "    with open(\"nli_data/\"+domain+\".jsonl\", \"w\") as eval_domain:\n",
    "        for ddict in split_domains[domain]:\n",
    "            jout = json.dumps(ddict) + '\\n'\n",
    "            eval_domain.write(jout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Q4J2pu60xHTd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1973it [00:01, 1732.69it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:03<00:00, 31.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: fiction\n",
      "Validation Loss: 0.609 | Accuracy: 76.89%\n",
      "F1: (79.41%, 72.18%, 78.77%) | Macro-F1: 76.79%\n",
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1945it [00:02, 897.18it/s] \n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 122/122 [00:05<00:00, 23.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: government\n",
      "Validation Loss: 0.483 | Accuracy: 81.85%\n",
      "F1: (84.45%, 77.97%, 82.68%) | Macro-F1: 81.70%\n",
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1955it [00:02, 938.39it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:05<00:00, 23.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: slate\n",
      "Validation Loss: 0.711 | Accuracy: 71.66%\n",
      "F1: (74.58%, 67.31%, 72.83%) | Macro-F1: 71.58%\n",
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1966it [00:02, 973.77it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:06<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: telephone\n",
      "Validation Loss: 0.596 | Accuracy: 77.82%\n",
      "F1: (81.08%, 71.63%, 80.33%) | Macro-F1: 77.68%\n",
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1976it [00:02, 935.44it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:05<00:00, 24.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: travel\n",
      "Validation Loss: 0.588 | Accuracy: 78.19%\n",
      "F1: (82.19%, 74.58%, 77.58%) | Macro-F1: 78.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(checkpoint)\n",
    "model.to(device)\n",
    "\n",
    "for domain in [\"fiction\", \"government\", \"slate\", \"telephone\", \"travel\"]:\n",
    "    \n",
    "    # Evaluate and save prediction results in each domain\n",
    "    dev_domain_dataset = NLIDataset(\"nli_data/\"+domain+\".jsonl\", tokenizer)\n",
    "    dev_loss, acc, f1_ent, f1_neu, f1_con = evaluate(dev_domain_dataset, model, device, batch_size, no_labels=False, result_save_file=\"predictions/\"+domain)\n",
    "    macro_f1 = (f1_ent + f1_neu + f1_con)/3\n",
    "    \n",
    "    print(f'Domain: {domain}')\n",
    "    print(f'Validation Loss: {dev_loss:.3f} | Accuracy: {acc*100:.2f}%')\n",
    "    print(f'F1: ({f1_ent*100:.2f}%, {f1_neu*100:.2f}%, {f1_con*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#6699FF; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#6699FF\"> Conclusion: </font> </b>\n",
    "<ul style=\"list-style-type:circle\"> \n",
    "<li> <b> For all domains, the F1 score is always higher for \"entailment\" then for \"contradiction\" and finally for \"neutral\".  </b></li>\n",
    "<li> <b> The model performs the best for the \"government\" domain with the highest accuracy and macro-F1. </b></li> \n",
    "<li> <b> The model performs the worst for the \"slate\" domain with the lowest accuracy and macro-F1, and the difference is rather marked with the other domains. </b></li> \n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at some examples to see if we can propose an explanation for these results...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of success for government:\n",
      "..................\n",
      "Example0: \n",
      " ---> premise: This site includes a list of all award winners and a searchable database of Government Executive articles. \n",
      " ---> hypothesis: The Government Executive articles housed on the website are not able to be searched.\n",
      "===> Label: contradiction,  Prediction: contradiction\n",
      "..................\n",
      "Example1: \n",
      " ---> premise: 5 The share of gross national saving used to replace depreciated capital has increased over the past 40 years. \n",
      " ---> hypothesis: Gross national saving was highest this year.\n",
      "===> Label: neutral,  Prediction: neutral\n",
      "..................\n",
      "Example2: \n",
      " ---> premise: So far, however, the number of mail pieces lost to alternative bill-paying methods is too small to have any material impact on First-Class volume. \n",
      " ---> hypothesis: The amount of lost mail is huge and really impacts mail volume\n",
      "===> Label: contradiction,  Prediction: contradiction\n",
      "..................\n",
      "Example3: \n",
      " ---> premise: Conversely, an increase in government saving adds to the supply of resources available for investment and may put downward pressure on interest rates. \n",
      " ---> hypothesis: Interest rates should increase to increase saving.\n",
      "===> Label: contradiction,  Prediction: contradiction\n",
      "\n",
      " ########## \n",
      "\n",
      "Examples of failures for government:\n",
      "..................\n",
      "Example0: \n",
      " ---> premise: We also have found that leading organizations strive to ensure that their core processes efficiently and effectively support mission-related outcomes. \n",
      " ---> hypothesis: Leading organizations want to be sure their employees are safe.\n",
      "===> Label: neutral,  Prediction: contradiction\n",
      "..................\n",
      "Example1: \n",
      " ---> premise: Table 2: Examples of BLM's, FHWA's, IRS's, and VBA's Customer Satisfaction Expectations for Senior Executive Performance \n",
      " ---> hypothesis: Senior Executive's have been studies on various aspects to reach the expectations.\n",
      "===> Label: entailment,  Prediction: neutral\n",
      "..................\n",
      "Example2: \n",
      " ---> premise: WHOLE LIFE POLICIES - Policies that provide insurance over the insured's entire life and the proceeds (face amount) are paid only upon death of the insured. \n",
      " ---> hypothesis: Whole life policies are a type of life insurance that only cover the insured person until retirement from the workforce.\n",
      "===> Label: contradiction,  Prediction: neutral\n",
      "..................\n",
      "Example3: \n",
      " ---> premise: (For more information on BLM's senior executive performance plans, see app. \n",
      " ---> hypothesis: BLM's performance plans are visible online.\n",
      "===> Label: entailment,  Prediction: neutral\n"
     ]
    }
   ],
   "source": [
    "# The \"government\" domain is the domain where the model perform the best\n",
    "# Let's look at some examples\n",
    "\n",
    "    # Save success and failures for government\n",
    "    \n",
    "succeed_government = []\n",
    "failed_government = []\n",
    "\n",
    "with jsonlines.open(\"predictions/government\", \"r\") as reader:\n",
    "    for gov in reader.iter():\n",
    "        \n",
    "        if gov[\"label\"]==gov[\"prediction\"]:\n",
    "            succeed_government.append(gov)\n",
    "        \n",
    "        elif gov[\"label\"]!=gov[\"prediction\"]:\n",
    "            failed_government.append(gov)\n",
    "\n",
    "    # Print examples\n",
    "\n",
    "print(\"Examples of success for government:\")\n",
    "for i in range(4):\n",
    "    print(\"..................\")\n",
    "    print(\"Example{}: \\n ---> premise: {} \\n ---> hypothesis: {}\".format(i, succeed_government[i][\"premise\"], succeed_government[i][\"hypothesis\"]))\n",
    "    print(\"===> Label: {},  Prediction: {}\".format(succeed_government[i][\"label\"], succeed_government[i][\"prediction\"]))\n",
    "\n",
    "print(\"\\n ########## \\n\")\n",
    "\n",
    "print(\"Examples of failures for government:\")\n",
    "for i in range(4):\n",
    "    print(\"..................\")\n",
    "    print(\"Example{}: \\n ---> premise: {} \\n ---> hypothesis: {}\".format(i, failed_government[i][\"premise\"], failed_government[i][\"hypothesis\"]))\n",
    "    print(\"===> Label: {},  Prediction: {}\".format(failed_government[i][\"label\"], failed_government[i][\"prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of success for slate:\n",
      "..................\n",
      "Example0: \n",
      " ---> premise: The new rights are nice enough \n",
      " ---> hypothesis: Everyone really likes the newest benefits \n",
      "===> Label: neutral,  Prediction: neutral\n",
      "..................\n",
      "Example1: \n",
      " ---> premise: If that investor were willing to pay extra for the security of limited downside, she could buy put options with a strike price of $98, which would lock in her profit on the shares at $18, less whatever the options cost. \n",
      " ---> hypothesis: THe strike price could be $8.\n",
      "===> Label: contradiction,  Prediction: contradiction\n",
      "..................\n",
      "Example2: \n",
      " ---> premise: 3)  Dare you rise to the occasion, like Raskolnikov, and reject the petty rules that govern lesser men? \n",
      " ---> hypothesis: Would you rise up and defeaat all evil lords in the town?\n",
      "===> Label: neutral,  Prediction: neutral\n",
      "..................\n",
      "Example3: \n",
      " ---> premise: Blue says Blumenthal claimed Clinton had told him that Lewinsky had made unwanted sexual advances. \n",
      " ---> hypothesis: Clinton said that Monica Lewinsky made unwanted sexual advances during her time as a journalist in the White House. \n",
      "===> Label: neutral,  Prediction: neutral\n",
      "\n",
      " ########## \n",
      "\n",
      "Examples of failures for slate:\n",
      "..................\n",
      "Example0: \n",
      " ---> premise: Mortifyingly enough, it is all  the difficulty, the laziness, the pathetic formlessness in youth, the round peg in the square hole, the whatever do you want? \n",
      " ---> hypothesis: Many youth are lazy.\n",
      "===> Label: entailment,  Prediction: neutral\n",
      "..................\n",
      "Example1: \n",
      " ---> premise: What's truly striking, though, is that Jobs has never really let this idea go. \n",
      " ---> hypothesis: Jobs never held onto an idea for long.\n",
      "===> Label: contradiction,  Prediction: entailment\n",
      "..................\n",
      "Example2: \n",
      " ---> premise: But when the cushion is spent in a year or two, or when the next recession arrives, the disintermediating voters will find themselves playing the roles of budget analysts and tax wonks. \n",
      " ---> hypothesis: The cushion will likely be spent in under two years.\n",
      "===> Label: entailment,  Prediction: contradiction\n",
      "..................\n",
      "Example3: \n",
      " ---> premise: She has exchanged a hollow life for a heightened life, and has tried to comprehend all its turns, get its possibilities. \n",
      " ---> hypothesis: She has chose to live a hollow life.\n",
      "===> Label: contradiction,  Prediction: entailment\n"
     ]
    }
   ],
   "source": [
    "# The \"slate\" domain is the domain where the model perform the worst\n",
    "# Let's look at some examples\n",
    "\n",
    "    # Save success and failures for slate\n",
    "    \n",
    "succeed_slate = []\n",
    "failed_slate = []\n",
    "\n",
    "with jsonlines.open(\"predictions/slate\", \"r\") as reader:\n",
    "    for slt in reader.iter():\n",
    "        \n",
    "        if slt[\"label\"]==slt[\"prediction\"]:\n",
    "            succeed_slate.append(slt)\n",
    "        \n",
    "        elif slt[\"label\"]!=slt[\"prediction\"]:\n",
    "            failed_slate.append(slt)\n",
    "\n",
    "    # Print examples\n",
    "\n",
    "print(\"Examples of success for slate:\")\n",
    "for i in range(4):\n",
    "    print(\"..................\")\n",
    "    print(\"Example{}: \\n ---> premise: {} \\n ---> hypothesis: {}\".format(i, succeed_slate[i][\"premise\"], succeed_slate[i][\"hypothesis\"]))\n",
    "    print(\"===> Label: {},  Prediction: {}\".format(succeed_slate[i][\"label\"], succeed_slate[i][\"prediction\"]))\n",
    "\n",
    "print(\"\\n ########## \\n\")\n",
    "\n",
    "print(\"Examples of failures for slate:\")\n",
    "for i in range(4):\n",
    "    print(\"..................\")\n",
    "    print(\"Example{}: \\n ---> premise: {} \\n ---> hypothesis: {}\".format(i, failed_slate[i][\"premise\"], failed_slate[i][\"hypothesis\"]))\n",
    "    print(\"===> Label: {},  Prediction: {}\".format(failed_slate[i][\"label\"], failed_slate[i][\"prediction\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#6699FF; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#6699FF\"> Conclusion: </font> </b>\n",
    "\n",
    "<b> When the model is wrong, we see that the error is rather small for the \"government\" domain, whereas for the \"slate\" domain, the label is often the opposite of the expected one. A hypothesis that could explain why the \"government\" domain performs well contrary to the \"slate\" domain is that the \"government\" domain uses a certain vocabulary that allows to deduce the \"orientation\" of the sentence from key words, while for the \"slate\" domain, the sentences have more often negations that make the \"orientation\" of the sentence is opposite to the key words. For example: For example1-success of \"government\", we have \"increased\" in premise and \"highest\" in hypothesis. Whereas for example1-failure of \"slate\", we see that there are many words that can change the \"orientation\" of the sentence with \"never\", \"let\" for premise and \"never\", \"held\" for hypothesis, which can confuse the model.\n",
    "</b>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NyMZ5E4-QxM"
   },
   "source": [
    "## **Task2: Identify Shortcuts**\n",
    "\n",
    "We aim to find some shortcuts that the model in 1.4 (under the first hyperparameter setting) has learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lCHLdaH_3vT"
   },
   "source": [
    "### **2.1 Word-Pair Pattern Extraction**\n",
    "\n",
    "We consider to extract simple word-pair patterns that the model may have learned from the NLI data. \n",
    "\n",
    "For this, we assume that a pair of words that occur in a premise-hypothesis sentence pair (one occurs in premise and the other occurs in hypothesis) may serve as a key indicator of the logical relationship between the premise and hypothesis sentences. For example:\n",
    "\n",
    ">- Premise: Consider the United States Postal Service.\n",
    ">- Hypothesis: Forget the United States Postal Service.\n",
    "\n",
    "Here the word-pair \"consider\" and \"forget\" determine that the premise and hypothesis have a *contradiction* relationship, so (consider, forget) --> *contradiction* might be a good pattern to learn.\n",
    "\n",
    "**Note:** \n",
    "- We do not consider the naive word pair patterns where the word from premise and the word from hypothesis are identical, e.g., (service, service) got from the above premise-hypothesis sentence pair.\n",
    "- We do not consider stop words neither, punctuations and words that contain special prefix '##', e.g., '##s' in the pattern extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7RKdt_-j_3vT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\felic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stop_words and puntuations to be removed from consideration in the pattern extraction\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.append('uh')\n",
    "\n",
    "import string\n",
    "puncs = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NovYRxv_3vU"
   },
   "source": [
    "**Complete `word_pair_extraction()` function in `shortcut.py` file.**\n",
    "\n",
    "The keys of the returned dictionary *word_pairs* should be **different word-pairs** appered in premise-hypothesis sentence pairs, i.e., (a word from the premise, a word from the hypothesis).\n",
    "\n",
    "The value of a word-pair key records the counts of entailment, neutral and contradiction predictions **made by the model** when the word-pair occurs, i.e., \\[#entailment_predictions, #neutral_predictions,  #contradiction_predictions\\].\n",
    "\n",
    "**Note:** Remember to remove naive word pairs (i.e., premise word identical to hypothesis word), stop_words, puntuations and words with special prefix '##' out of consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTHt1frZ_3vU"
   },
   "source": [
    "### **2.2 Distill Potentially Useful Patterns**\n",
    "\n",
    "Find and print the **top-100** word-pairs that are associated with the **largest total number** of model predictions, which might contain frequently used patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IFZm6tFJFmlg"
   },
   "outputs": [],
   "source": [
    "from shortcut import word_pair_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "TUyal5mW_3vU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('legal', 'services'), ('postal', 'service'), ('could', 'would'), ('children', 'kids'), ('like', 'lot'), ('know', 'time'), ('like', 'think'), ('one', 'year'), ('know', 'like'), ('one', 'two'), ('like', 'one'), ('know', 'think'), ('like', 'yeah'), ('one', 'people'), ('people', 'would'), ('get', 'know'), ('many', 'people'), ('last', 'year'), ('know', 'money'), ('know', 'would'), ('ca', 'da'), ('time', 'would'), ('like', 'really'), ('year', 'years'), ('know', 'people'), ('use', 'used'), ('many', 'one'), ('get', 'going'), ('think', 'would'), ('bad', 'good'), ('time', 'well'), ('never', 'yeah'), ('get', 'one'), ('like', 'people'), ('new', 'york'), ('l', 'state'), ('help', 'legal'), ('legal', 'state'), ('cost', 'costs'), ('one', 'yeah'), ('time', 'yeah'), ('one', 'would'), ('income', 'people'), ('last', 'years'), ('know', 'never'), ('go', 'know'), ('never', 'well'), ('said', 'told'), ('last', 'one'), ('good', 'one'), ('good', 'well'), ('get', 'well'), ('like', 'way'), ('well', 'would'), ('know', 'lot'), ('people', 'well'), ('know', 'take'), ('good', 'like'), ('get', 'yeah'), ('good', 'would'), ('know', 'one'), ('really', 'would'), ('get', 'want'), ('really', 'think'), ('know', 'well'), ('right', 'well'), ('people', 'think'), ('like', 'well'), ('national', 'saving'), ('aid', 'legal'), ('may', 'might'), ('lot', 'people'), ('like', 'never'), ('think', 'yeah'), ('good', 'time'), ('good', 'know'), ('long', 'time'), ('get', 'think'), ('get', 'like'), ('u', 'us'), ('states', 'us'), ('united', 'us'), ('case', 'studies'), ('saving', 'savings'), ('legal', 'system'), ('lot', 'well'), ('know', 'much'), ('like', 'would'), ('believe', 'think'), ('know', 'way'), ('know', 'really'), ('like', 'see'), ('get', 'go'), ('know', 'year'), ('know', 'long'), ('get', 'guess'), ('go', 'well'), ('one', 'think'), ('lot', 'much'), ('like', 'much')]\n"
     ]
    }
   ],
   "source": [
    "# all your saved model prediction results in 1.2 Fine-Grained Validation\n",
    "prediction_files = [\"predictions/fiction\", \"predictions/government\", \"predictions/slate\", \"predictions/telephone\", \"predictions/travel\"]\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "word_pairs, pairs_pred_files = word_pair_extraction(prediction_files, tokenizer)\n",
    "\n",
    "# find top-100 word-pairs associated with the largest total number of model predictions\n",
    "sum_word_pairs = {k:sum(v) for k,v in word_pairs.items()}\n",
    "top_100_freq_pairs = list(dict(sorted(sum_word_pairs.items(), key=lambda item: item[1], reverse=True)).keys())[0:100]\n",
    "\n",
    "print(top_100_freq_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz25EvuI_3vU"
   },
   "source": [
    "**Among the top-100 frequent word-pairs above**, find out the **top-5** word-pairs whose occurances **most likely** lead to *entailment* predictions (entailment patterns), and the **top-5** word-pairs whose occurances **most likely** lead to *contradiction* predictions (contradiction patterns).\n",
    "\n",
    "**Explain your rules for finding these word pairs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#66CC66; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#66CC66\"> Method: </font> </b>\n",
    "<ul style=\"list-style-type:circle\"> \n",
    "<li> <b> We built the following dictionary: {word-pair: [#entailment_predictions, #neutral_predictions, #contradiction_predictions]}.  </b></li>\n",
    "<li> <b> To find the top-100 word-pairs, we compute #entailment_predictions+#neutral_predictions+#contradiction_predictions and keep the 100 word-pairs with the highest sums. </b></li> \n",
    "<li> <b> Then among these 100 word-pairs selected, we keep only the 5 word-pairs with #entailment_predictions the highest on one side, and the 5 word-pairs with #contradiction_predictions the highest on the other side. We assume that this allows us to have the top-5 word-pairs whose occurrences most likely lead to entailment predictions (entailment patterns) on one side, and the top-5 word-pairs whose occurrences most likely lead to contradiction predictions on the other side. </b></li> \n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Yq2cVOaWTEYw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Patterns:\n",
      "[('legal', 'services'), ('like', 'lot'), ('postal', 'service'), ('children', 'kids'), ('like', 'think')]\n",
      "Contradiction Patterns:\n",
      "[('never', 'yeah'), ('legal', 'services'), ('bad', 'good'), ('know', 'never'), ('postal', 'service')]\n"
     ]
    }
   ],
   "source": [
    "# find top-5 entailment and contradiction patterns\n",
    "    # entailment\n",
    "entailment_t100 = {key: word_pairs[key][0] for key in top_100_freq_pairs}\n",
    "top_5_entailment = list(dict(sorted(entailment_t100.items(), key=lambda item: item[1], reverse=True)).keys())[0:5]\n",
    "    # contradict\n",
    "contradict_t100 = {key: word_pairs[key][2] for key in top_100_freq_pairs}\n",
    "top_5_contradict = list(dict(sorted(contradict_t100.items(), key=lambda item: item[1], reverse=True)).keys())[0:5]\n",
    "\n",
    "# print\n",
    "print(\"Entailment Patterns:\")\n",
    "print(top_5_entailment)\n",
    "print(\"Contradiction Patterns:\")\n",
    "print(top_5_contradict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#6699FF; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#6699FF\"> Conclusion: </font> </b>\n",
    "<ul style=\"list-style-type:circle\"> \n",
    "<li> <b> If some patterns seem logical like ('bad', 'good') for \"contradiction patterns\", others seem much less obvious like ('like', 'lot') for \"entailment patterns\". Moreover, we find the same pattern ('postal', 'service') for \"contradiction\" and \"entailment\". </b></li>\n",
    "<li> <b> The fact to have some patterns that seem \"weird\" could suggest that the model relies on some shorcuts that are not relevant to make its prediction, but of course it remains difficult to judge without more context. </b></li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjVti-vL_3vV"
   },
   "source": [
    "### **2.3 Case Study**\n",
    "\n",
    "Find out and study **4 representative** cases where the pattern that you have found in 2.2 **fails**, e.g., the premise-hypothesis sentence pair contains ('good', 'bad'), but has an *entailment* gold label.\n",
    "\n",
    "**Based on your case study, explain the limitations of the word-pair patterns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2XyPmb01_3vV"
   },
   "outputs": [],
   "source": [
    "# Find 4 representative cases where the pattern in 2.2 fails\n",
    "failed_cases = []\n",
    "for l in pairs_pred_files:\n",
    "    if len(failed_cases) < 4:\n",
    "        if (bool(set(l['pairs']) & set(top_5_entailment+top_5_contradict))) \\\n",
    "        & (l['label']!=l['prediction']) \\\n",
    "        & (l['label']!='neutral') \\\n",
    "        & (l['prediction']!='neutral'):\n",
    "            failed_cases.append(l)\n",
    "    else:\n",
    "        break     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'I feel, though, that I should like to point out to you once more the risks you are running, especially if you pursue the course you indicate.',\n",
       " 'hypothesis': 'I do not think that you understand the risks you are taking.',\n",
       " 'pairs': [('pursue', 'think'),\n",
       "  ('point', 'understand'),\n",
       "  ('risks', 'though'),\n",
       "  ('especially', 'understand'),\n",
       "  ('pursue', 'understand'),\n",
       "  ('course', 'understand'),\n",
       "  ('risks', 'understand'),\n",
       "  ('like', 'think'),\n",
       "  ('point', 'risks'),\n",
       "  ('feel', 'taking'),\n",
       "  ('especially', 'risks'),\n",
       "  ('pursue', 'risks'),\n",
       "  ('course', 'risks'),\n",
       "  ('indicate', 'understand'),\n",
       "  ('running', 'understand'),\n",
       "  ('feel', 'think'),\n",
       "  ('indicate', 'risks'),\n",
       "  ('like', 'understand'),\n",
       "  ('point', 'taking'),\n",
       "  ('especially', 'taking'),\n",
       "  ('think', 'though'),\n",
       "  ('taking', 'though'),\n",
       "  ('course', 'taking'),\n",
       "  ('risks', 'taking'),\n",
       "  ('risks', 'running'),\n",
       "  ('point', 'think'),\n",
       "  ('like', 'risks'),\n",
       "  ('especially', 'think'),\n",
       "  ('feel', 'understand'),\n",
       "  ('course', 'think'),\n",
       "  ('risks', 'think'),\n",
       "  ('indicate', 'taking'),\n",
       "  ('running', 'taking'),\n",
       "  ('pursue', 'taking'),\n",
       "  ('though', 'understand'),\n",
       "  ('feel', 'risks'),\n",
       "  ('indicate', 'think'),\n",
       "  ('running', 'think'),\n",
       "  ('like', 'taking')],\n",
       " 'domain': 'fiction',\n",
       " 'label': 'entailment',\n",
       " 'prediction': 'contradiction'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case 1\n",
    "failed_cases[0]  # see ('like', 'think')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': \"Waldemar Szary, a food technician at the OSM 'Paziocha', was having a very bad day - the kind of a very bad day, which normally comes after one of those very good days.\",\n",
       " 'hypothesis': 'The kind of day that Waldemar Szary was having was not a good one, at all. ',\n",
       " 'pairs': [('bad', 'day'),\n",
       "  ('technician', 'wal'),\n",
       "  ('good', 'kind'),\n",
       "  ('bad', 'good'),\n",
       "  ('day', 'food'),\n",
       "  ('day', 'kind'),\n",
       "  ('paz', 'wal'),\n",
       "  ('comes', 'kind'),\n",
       "  ('food', 'one'),\n",
       "  ('one', 'wal'),\n",
       "  ('good', 'wal'),\n",
       "  ('bad', 'kind'),\n",
       "  ('one', 'technician'),\n",
       "  ('good', 'technician'),\n",
       "  ('food', 'good'),\n",
       "  ('comes', 'wal'),\n",
       "  ('good', 'paz'),\n",
       "  ('days', 'wal'),\n",
       "  ('kind', 'os'),\n",
       "  ('good', 'normally'),\n",
       "  ('days', 'one'),\n",
       "  ('kind', 'wal'),\n",
       "  ('one', 'os'),\n",
       "  ('good', 'os'),\n",
       "  ('comes', 'day'),\n",
       "  ('kind', 'technician'),\n",
       "  ('day', 'os'),\n",
       "  ('food', 'kind'),\n",
       "  ('kind', 'paz'),\n",
       "  ('day', 'wal'),\n",
       "  ('good', 'one'),\n",
       "  ('days', 'good'),\n",
       "  ('kind', 'normally'),\n",
       "  ('one', 'paz'),\n",
       "  ('day', 'one'),\n",
       "  ('day', 'technician'),\n",
       "  ('comes', 'one'),\n",
       "  ('day', 'paz'),\n",
       "  ('food', 'wal'),\n",
       "  ('day', 'normally'),\n",
       "  ('bad', 'wal'),\n",
       "  ('normally', 'wal'),\n",
       "  ('day', 'days'),\n",
       "  ('kind', 'one'),\n",
       "  ('days', 'kind'),\n",
       "  ('bad', 'one'),\n",
       "  ('day', 'good'),\n",
       "  ('normally', 'one'),\n",
       "  ('os', 'wal'),\n",
       "  ('comes', 'good')],\n",
       " 'domain': 'fiction',\n",
       " 'label': 'entailment',\n",
       " 'prediction': 'contradiction'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case 2\n",
    "failed_cases[1]  # see ('bad', 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': \"If she didn't like her restaurant so much, the woman'd be high-up in Applied by now.\",\n",
       " 'hypothesis': 'She liked her restaurant a lot.',\n",
       " 'pairs': [('lot', 'woman'),\n",
       "  ('much', 'restaurant'),\n",
       "  ('applied', 'restaurant'),\n",
       "  ('liked', 'much'),\n",
       "  ('liked', 'restaurant'),\n",
       "  ('like', 'liked'),\n",
       "  ('lot', 'much'),\n",
       "  ('high', 'liked'),\n",
       "  ('lot', 'restaurant'),\n",
       "  ('like', 'lot'),\n",
       "  ('high', 'lot'),\n",
       "  ('applied', 'liked'),\n",
       "  ('restaurant', 'woman'),\n",
       "  ('like', 'restaurant'),\n",
       "  ('liked', 'woman'),\n",
       "  ('high', 'restaurant'),\n",
       "  ('applied', 'lot')],\n",
       " 'domain': 'fiction',\n",
       " 'label': 'entailment',\n",
       " 'prediction': 'contradiction'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case 3\n",
    "failed_cases[2]  # see ('like', 'lot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'LASNNY is one of the oldest and most cost-effective legal services organizations in the United States.',\n",
       " 'hypothesis': 'LASNNY is an old legal services organization.',\n",
       " 'pairs': [('cost', 'old'),\n",
       "  ('oldest', 'services'),\n",
       "  ('one', 'organization'),\n",
       "  ('las', 'united'),\n",
       "  ('legal', 'united'),\n",
       "  ('las', 'organization'),\n",
       "  ('legal', 'organization'),\n",
       "  ('effective', 'services'),\n",
       "  ('las', 'old'),\n",
       "  ('effective', 'legal'),\n",
       "  ('las', 'legal'),\n",
       "  ('cost', 'organization'),\n",
       "  ('cost', 'legal'),\n",
       "  ('effective', 'las'),\n",
       "  ('old', 'one'),\n",
       "  ('services', 'states'),\n",
       "  ('organization', 'united'),\n",
       "  ('organizations', 'services'),\n",
       "  ('organization', 'organizations'),\n",
       "  ('old', 'states'),\n",
       "  ('one', 'services'),\n",
       "  ('las', 'organizations'),\n",
       "  ('legal', 'organizations'),\n",
       "  ('old', 'oldest'),\n",
       "  ('las', 'services'),\n",
       "  ('legal', 'services'),\n",
       "  ('cost', 'services'),\n",
       "  ('old', 'united'),\n",
       "  ('cost', 'las'),\n",
       "  ('organization', 'services'),\n",
       "  ('las', 'one'),\n",
       "  ('legal', 'one'),\n",
       "  ('oldest', 'organization'),\n",
       "  ('las', 'states'),\n",
       "  ('legal', 'states'),\n",
       "  ('services', 'united'),\n",
       "  ('las', 'oldest'),\n",
       "  ('old', 'organizations'),\n",
       "  ('legal', 'oldest'),\n",
       "  ('old', 'services'),\n",
       "  ('effective', 'organization'),\n",
       "  ('organization', 'states'),\n",
       "  ('effective', 'old'),\n",
       "  ('legal', 'old')],\n",
       " 'domain': 'government',\n",
       " 'label': 'entailment',\n",
       " 'prediction': 'contradiction'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case 4\n",
    "failed_cases[3]  # see ('legal', 'services')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#6699FF; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#6699FF\"> Conclusion: </font> </b>\n",
    "\n",
    "<b> The problem with shorcuts is that they are mainly based on two words without taking into account the context around them which can strongly change the direction of the sentence. For example, the presence of a negation can change the meaning of the pattern as we can see with these two examples: \"should like VS do not think\" or \"very bad day VS was not a good one\". Another example of context is the presence of an implicit meaning like with \"If she didn't like VS liked a lot\". </b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yND0DEfT-eXn"
   },
   "source": [
    "## **Task3: Annotate New Data**\n",
    "\n",
    "To check the robustness of developed model, **some additional sets of test data** are collected, which contain NLI samples that are out of the domains of the training and validation data.\n",
    "\n",
    "However, the test data does not have gold labels of the relationships between premise and hypothesis sentences, i.e., all the labels are marked as *hidden*. **We consider to annotate the data by ourselves.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQNXrRHr_3vV"
   },
   "source": [
    "### **3.1 Write an Annotation Guideline**\n",
    "\n",
    "Imagine that you are going to assign this annotation task to a crowdsourcing worker, who is completely not familiar with computer science and NLP. Think about how you are going to explain this annotation task to him in order to guide him do a decent job. Write an annotation guideline for such a worker who are going to do this task for you.\n",
    "\n",
    "**Note:** You should come up with your own guideline without the help of your partner(s) in later Task 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#66CC66; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#66CC66\"> Method: Annotation guideline: </font> </b>\n",
    "\n",
    "<b> <i> Instructions: <br>\n",
    "For each sample, two sentences are present each time: the \"premise\" sentence and the \"hypothesis\" sentence. The task is to label each sample with one of the three labels: \"entailment\" or \"contradiction\" or \"neutral\". <br>\n",
    "In a general way, the sample should be labeled \"entailment\" if the \"hypothesis\" sentence follows from the \"premise\" sentence, it should be labeled \"contradiction\" if the \"hypothesis\" sentence opposes the \"premise\" sentence, and it should be labeled \"neutral\" if the \"hypothesis\" sentence has no direct link with the \"premise\" sentence. </i> </b> <br>\n",
    "\n",
    "<b> More specifically: </b>\n",
    "\n",
    "<ul style=\"list-style-type:circle\"> \n",
    "<li> <b> Look to see if there is even one contradictory element between the \"premise\" and the \"hypothesis\". If there is, then the label will necessarily be \"contradiction\", regardless of the other part(s) of the sentence. </b> Example: \"premise\": \"The sun is shining today and it is warm despite a few clouds\"; \"hypothesis\": \"The sun is shining today with its warmth, and there are no clouds on the horizon.\" => \"contradiction\".</li>\n",
    "<li> <b> If no contradictory element is present but the \"hypothesis\" has one or more additions not directly deducible from the \"premise\" then the label will necessarily be \"neutral\" even if the other part(s) of the sentence are deducible. </b> Example: \"premise\": \"The sun is shining today and it is warm despite a few clouds\"; \"hypothesis\": \"The sun is shining today with its warmth, and the birds are singing.\" => \"neutral\". </li>\n",
    "<li> <b> If no contradictory elements and no additions are present in the \"hypothesis\", i.e. if everything in the \"hypothesis\" is deductible from the \"premise\", then the label will be \"entailment\". </b> Example: \"premise\": \"The sun is shining today and it's warm despite a few clouds\"; \"hypothesis\": \"The sun is shining today with its warmth.\" => \"entailement\". </li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBWK4Bw__3vV"
   },
   "source": [
    "### **3.2 Annotate Your 100 Datapoints with Partner(s)**\n",
    "\n",
    "Annotate your 100 test datapoints with your partner(s), by editing the value of the key \"label_student1\", \"label_student2\" and \"label_student3\" (if you are in a group of three students) in each datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cczFQm0eHknv"
   },
   "source": [
    "**Note:** \n",
    "- You can download the assigned annotation file (`<your-testset-id>.jsonl`) by [this link](https://drive.google.com/drive/folders/146ExExmpnSUayu6ArGiN5gQzCPJp0myB?usp=share_link)\n",
    "- Please find your annotation partner according to the \"Student Pairing List for A2 Task3\" shared on Ed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWhjTn2fQ5YE"
   },
   "source": [
    "**Name your annotated file as `<index>-<sciper_number>.jsonl`.** \n",
    "\n",
    "For example, if you get `01.jsonl` to annotate, you should name your deliverable as `01-<your_sciper_number>.jsonl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyP0GtHe_3vW"
   },
   "source": [
    "### **3.3 Agreement Measure**\n",
    "\n",
    "Based on your and your partner's annotations on the 100 test datapoints in 3.2, calculate the [Cohen's Kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa) or [Krippendorff's Alpha](https://github.com/pln-fing-udelar/fast-krippendorff) (if you are in a group of three students) between the annotators. Discuss the agreement measure results.\n",
    "\n",
    "**Note:** Cohen's Kappa or Krippendorff's Alpha interpretation\n",
    "\n",
    "0: No Agreement\n",
    "\n",
    "0 ~ 0.2: Slight Agreement\n",
    "\n",
    "0.2 ~ 0.4: Fair Agreement\n",
    "\n",
    "0.4 ~ 0.6: Moderate Agreement\n",
    "\n",
    "0.6 ~ 0.8: Substantial Agreement\n",
    "\n",
    "0.8 ~ 1.0: Near Perfect Agreement\n",
    "\n",
    "1.0: Perfect Agreement\n",
    "\n",
    "> **Questions**: What is your interpretation of Cohen's Kappa or Krippendorff's Alpha value according to the above mapping? Which kind of disagreements are most frequently happen between you and your partner(s), i.e., *entailment* vs. *neutral*, *entailment* vs. *contradiction*, or *neutral* vs. *contradiction*? For the second question, give some examples to explain why that is the case. Are there possible ways to address the disagrrements between two annotators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CpS6VOiX_3vW"
   },
   "outputs": [],
   "source": [
    "# Open jsonl file and record disagreements vs agreements\n",
    "\n",
    "annotations = {\"premise\":[], \"hypothesis\":[], \"domain\":[], \"label_student1\":[], \"label_student2\":[]}\n",
    "\n",
    "disagreements_number = {\"eVSn\":0, \"eVSc\":0, \"nVSc\":0}\n",
    "agreements_number = {\"e\":0, \"c\":0, \"n\":0}\n",
    "\n",
    "disagreements_eVSn = {\"premise\":[], \"hypothesis\":[], \"label_student1\":[], \"label_student2\":[]}\n",
    "disagreements_eVSc = {\"premise\":[], \"hypothesis\":[], \"label_student1\":[], \"label_student2\":[]}\n",
    "disagreements_nVSc = {\"premise\":[], \"hypothesis\":[], \"label_student1\":[], \"label_student2\":[]}\n",
    "\n",
    "with jsonlines.open(\"nli_data/27-284220.jsonl\", \"r\") as reader:\n",
    "    for ann in reader.iter():\n",
    "        \n",
    "        annotations[\"premise\"].append(ann[\"premise\"])\n",
    "        annotations[\"hypothesis\"].append(ann[\"hypothesis\"])\n",
    "        annotations[\"domain\"].append(ann[\"domain\"])\n",
    "        annotations[\"label_student1\"].append(ann[\"label_student1\"])\n",
    "        annotations[\"label_student2\"].append(ann[\"label_student2\"])\n",
    "        \n",
    "        if (ann[\"label_student1\"], ann[\"label_student2\"]) in [(\"entailment\", \"neutral\"), (\"neutral\", \"entailment\")]:\n",
    "            disagreements_number[\"eVSn\"] += 1\n",
    "            disagreements_eVSn[\"premise\"].append(ann[\"premise\"])\n",
    "            disagreements_eVSn[\"hypothesis\"].append(ann[\"hypothesis\"])\n",
    "            disagreements_eVSn[\"label_student1\"].append(ann[\"label_student1\"])\n",
    "            disagreements_eVSn[\"label_student2\"].append(ann[\"label_student2\"])\n",
    "                \n",
    "        elif (ann[\"label_student1\"], ann[\"label_student2\"]) in [(\"entailment\", \"contradictory\"), (\"contradictory\", \"entailment\")]:\n",
    "            disagreements_number[\"eVSc\"] += 1\n",
    "            disagreements_eVSc[\"premise\"].append(ann[\"premise\"])\n",
    "            disagreements_eVSc[\"hypothesis\"].append(ann[\"hypothesis\"])\n",
    "            disagreements_eVSc[\"label_student1\"].append(ann[\"label_student1\"])\n",
    "            disagreements_eVSc[\"label_student2\"].append(ann[\"label_student2\"])\n",
    "                        \n",
    "        elif (ann[\"label_student1\"], ann[\"label_student2\"]) in [(\"neutral\", \"contradictory\"), (\"contradictory\", \"neutral\")]:\n",
    "            disagreements_number[\"nVSc\"] += 1\n",
    "            disagreements_nVSc[\"premise\"].append(ann[\"premise\"])\n",
    "            disagreements_nVSc[\"hypothesis\"].append(ann[\"hypothesis\"])\n",
    "            disagreements_nVSc[\"label_student1\"].append(ann[\"label_student1\"])\n",
    "            disagreements_nVSc[\"label_student2\"].append(ann[\"label_student2\"])\n",
    "                      \n",
    "        elif (ann[\"label_student1\"]==ann[\"label_student2\"]) & (ann[\"label_student1\"]==\"entailment\"):\n",
    "            agreements_number[\"e\"] += 1\n",
    "                              \n",
    "        elif (ann[\"label_student1\"]==ann[\"label_student2\"]) & (ann[\"label_student1\"]==\"contradictory\"):\n",
    "            agreements_number[\"c\"] += 1\n",
    "                                      \n",
    "        elif (ann[\"label_student1\"]==ann[\"label_student2\"]) & (ann[\"label_student1\"]==\"neutral\"):\n",
    "            agreements_number[\"n\"] += 1\n",
    "\n",
    "# Calculate the Cohen's Kappa             \n",
    "CohenKappa = cohen_kappa_score(annotations[\"label_student1\"], annotations[\"label_student2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cohen's Kappa is: 0.64.\n",
      "===============\n",
      "Disagreements:\n",
      "There is 14 eVSn, 1 eVSc, 9 nVSc.\n",
      "===============\n",
      "Agreements:\n",
      "There is 20 e, 35 c, 21 n.\n"
     ]
    }
   ],
   "source": [
    "print(\"The Cohen's Kappa is: {:.2f}.\".format(CohenKappa))\n",
    "print(\"===============\")\n",
    "print(\"Disagreements:\")\n",
    "print(\"There is {} eVSn, {} eVSc, {} nVSc.\".format(disagreements_number[\"eVSn\"], disagreements_number[\"eVSc\"], disagreements_number[\"nVSc\"]))\n",
    "print(\"===============\")\n",
    "print(\"Agreements:\")\n",
    "print(\"There is {} e, {} c, {} n.\".format(agreements_number[\"e\"], agreements_number[\"c\"], agreements_number[\"n\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#6699FF; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#6699FF\"> Conclusion: </font> </b>\n",
    "<ul style=\"list-style-type:circle\"> \n",
    "<li> <b> Cohen's Kappa is 0.64. It is thus between 0.6 and 0.8, meaning a \"substantial agreement\". We can therefore consider here that the agreement between the two annotators is correct even if some disagreements are present. </b></li>\n",
    "<li> <b> The most disagreements are at the level of \"entailment\" VS \"neutral\". This seems quite logical because my own rule is to favor the \"neutral\" label if there is ever a non-deductible addition and even if everything else is deductible. This rule can be different for the other annotator without it seeming weird, and could be for example \"Put the label \"entailment\" if there is a deductible part and no contradictory part, even if a part of the sentence is not deductible.\". </b></li>\n",
    "<li> <b> The most common labels are \"contradiction\", which again makes sense because the presence of a contradiction is easily noticed, and the rule \"You only need one contradiction to put the label \"contradiction\".\" seems quite logical. </b></li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at some examples where the annotations disagree in the case of entailment VS neutral:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of disagreements eVSn:\n",
      "..................\n",
      "Example0: \n",
      " ---> premise: Gregory uses stream in the way normal in England; Giles consistently refers to gens in the hills of central Australia, though glen is not current (outside place-names) in contemporary Australian English. \n",
      " ---> hypothesis: Glen is not contemporary Australian English right now.\n",
      "===> Label other student: neutral,  My label: entailment\n",
      "..................\n",
      "Example1: \n",
      " ---> premise: A rough comparison yields  \n",
      " ---> hypothesis: A tough juxtaposition results in \n",
      "===> Label other student: entailment,  My label: neutral\n",
      "..................\n",
      "Example2: \n",
      " ---> premise: At 8:47, seconds after the impact of American 11, United 175's transponder code changed, and then changed again. \n",
      " ---> hypothesis: After impact of American 11, the transponder code was altered on United 175.\n",
      "===> Label other student: neutral,  My label: entailment\n",
      "..................\n",
      "Example3: \n",
      " ---> premise:  Copy that, sir. \n",
      " ---> hypothesis: I understand what you want.\n",
      "===> Label other student: neutral,  My label: entailment\n"
     ]
    }
   ],
   "source": [
    "# Examples of disagreements entailment VS neutral\n",
    "\n",
    "print(\"Examples of disagreements eVSn:\")\n",
    "for i in range(4):\n",
    "    print(\"..................\")\n",
    "    print(\"Example{}: \\n ---> premise: {} \\n ---> hypothesis: {}\".format(i, disagreements_eVSn[\"premise\"][i], disagreements_eVSn[\"hypothesis\"][i]))\n",
    "    print(\"===> Label other student: {},  My label: {}\".format(disagreements_eVSn[\"label_student1\"][i], disagreements_eVSn[\"label_student2\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#6699FF; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#6699FF\"> Conclusion: </font> </b>\n",
    "\n",
    "<b> The disagreements seem to be on the sentences where the label to put is quite ambiguous. The interpretation of certain words may differ slightly between the two annotators and lead to a slightly different label. For example, can \"comparison\" and \"juxtaposition\" in example 1 really be considered as having the same meaning? Here the disagreements seem to be quite subtle. One way to reconcile the labels would be to have a much more precise and common guideline annotation, since here the differences seem to be more about small details. In addition, it seems that the choice between \"entailment\" and \"neutral\" especially should be clarified. </b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff12-RLf_3vW"
   },
   "source": [
    "### **3.4 Robustness Check**\n",
    "\n",
    "Take into account both your and your partner's annotations, determine the final labels of the 100 test datapoints, by editing the value of the key \"label\" in each of your datapoint.\n",
    "\n",
    "Evaluate the performance of your developed model in 1.4 (still under the first hyperparameter setting) on your annotated 100 test datapoints, and compare with the model performance on the validation set.\n",
    "\n",
    "> **Question**: Do you think that your developed model has a good robuestness of handling out-of-domain NLI predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#66CC66; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#66CC66\"> Method: Determine the final labels: </font> </b>\n",
    "\n",
    "<b> To fill in the labels, I will choose to take my labels each time. As long as Cohen's Kappa is high enough, and in the examples above I prefer my labels, this choice seems reasonable. </b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LowJ05h6ypaA"
   },
   "outputs": [],
   "source": [
    "# Fill final labels with labels of student2 (my labels)\n",
    "\n",
    "final_labels = []\n",
    "\n",
    "with jsonlines.open(\"nli_data/27-284220.jsonl\", \"r\") as reader:\n",
    "    for ann in reader.iter():\n",
    "        ann[\"label\"] = ann[\"label_student2\"]\n",
    "        if ann[\"label\"]==\"contradictory\":\n",
    "            ann[\"label\"] = \"contradiction\"\n",
    "        final_labels.append(ann)\n",
    "\n",
    "with open(\"nli_data/27-284220_final.jsonl\", \"w\") as writer:\n",
    "    for dict_ in final_labels:\n",
    "        jout = json.dumps(dict_) + '\\n'\n",
    "        writer.write(jout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 797.99it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.466 | Accuracy: 81.00%\n",
      "F1: (89.23%, 66.67%, 82.76%) | Macro-F1: 79.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the previous model with a learning_rate of 2e-5 on our new annotated file\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "warmup_percent = 0.3\n",
    "checkpoint = 'runs/lr{}-warmup{}'.format(learning_rate, warmup_percent)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(checkpoint)\n",
    "model.to(device)\n",
    "\n",
    "annotation_dataset = NLIDataset(\"nli_data/27-284220_final.jsonl\", tokenizer)\n",
    "val_loss, acc, f1_ent, f1_neu, f1_con = evaluate(annotation_dataset, model, device, batch_size, no_labels=False, result_save_file=\"predictions/our-ann\")\n",
    "macro_f1 = (f1_ent + f1_neu + f1_con)/3\n",
    "    \n",
    "print(f'Validation Loss: {val_loss:.3f} | Accuracy: {acc*100:.2f}%')\n",
    "print(f'F1: ({f1_ent*100:.2f}%, {f1_neu*100:.2f}%, {f1_con*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#6699FF; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#6699FF\"> Conclusion: </font> </b>\n",
    "<ul style=\"list-style-type:circle\"> \n",
    "<li> <b> When we evaluate our model on our new annotation file, the accuracy (81.0%) and the macro-F1 (79.6%) are relatively high. Their values are slightly higher than the one on the validation set (we had 77.0% for the accuracy and 76.8% for the macro-F1 for epoch3). Moreover the values are almost as high as for the best domain (we had 81.9% for the accuracy and 81.7% for the macro-F1 for the best domain, i.e. the \"government\" domain). </b></li>\n",
    "<li> <b> We can therefore deduce that our developed model has a good robuestness of handling out-of-domain NLI predictions. </b></li>\n",
    "<li> <b> NB: we find again the best F1 for \"entailment\", then for \"contradiction\", and then for \"neutral\". </b></li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4wuRpHt-rQF"
   },
   "source": [
    "## **Task4: Data Augmentation**\n",
    "\n",
    "Finally, we consider to use a data augmentation method to create more training data, and use the augmented data to improve the model performance. The data augmentation method we are going to use is [EDA](https://aclanthology.org/D19-1670/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEtgwKJt0kfO"
   },
   "source": [
    "### **4.1 EDA: Easy Data Augmentation algorithm for Text**\n",
    "\n",
    "For this section, we will need to implement the most simple data augmentation techniques on textual sentences, including **SR** (Synonym Replacement), **RD** (Random Deletion), **RS** (Random Swap), **RI** (Random Insertion). \n",
    "\n",
    "You should complete all the functions in `eda.py` script, and you can test them with a simple testcase by running the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djFbjk31AR0M"
   },
   "source": [
    "- **Synonym Replacement (SR)**\n",
    "> In Synonym Replacement, we randomly replace some words in the sentence with their synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2ZExbEb03IN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\felic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\felic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "  \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyJi-zYyIqsq"
   },
   "source": [
    "You can test whether you get the synonyms right and see an example with synonym replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMRmcZtx81R4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\felic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The synonyms for the word \"task\" are:  ['job', 'labor', 'chore', 'undertaking', 'project', 'tax']\n"
     ]
    }
   ],
   "source": [
    "from eda import get_synonyms\n",
    "from testA2 import test_get_synonyms\n",
    "\n",
    "test_get_synonyms(get_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjFGy4DLFziY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example of Synonym Replacement: hey gentleman how are you doing\n"
     ]
    }
   ],
   "source": [
    "from eda import synonym_replacement\n",
    "\n",
    "print(f\" Example of Synonym Replacement: {synonym_replacement('hey man how are you doing',3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfdHwyxaJXUn"
   },
   "source": [
    "- **Random Deletion (RD)**\n",
    "\n",
    "> In Random Deletion, we randomly delete a word if a uniformly generated number between 0 and 1 is smaller than a pre-defined threshold. This allows for a random deletion of some words of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "76bVm640Msa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example of Random Deletion: hey man are\n"
     ]
    }
   ],
   "source": [
    "from eda import random_deletion\n",
    "\n",
    "print(f\" Example of Random Deletion: {random_deletion('hey man how are you doing', p=0.3, max_deletion_n=3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ajqx9cABNk5a"
   },
   "source": [
    "- **Random Swap (RS)**\n",
    "> In Random Swap, we randomly swap the order of two words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6Vuott-vQn6W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example of Random Swap: hey man are how you doing\n"
     ]
    }
   ],
   "source": [
    "from eda import swap_word\n",
    "\n",
    "print(f\" Example of Random Swap: {swap_word('hey man how are you doing')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSpcGqdfQ78_"
   },
   "source": [
    "- **Random Insertion (RI)**\n",
    "> Finally, in Random Insertion, we randomly insert synonyms of a word at a random position.\n",
    "> Data augmentation operations should not change the true label of a sentence, as that would introduce unnecessary noise into the data. Inserting a synonym of a word in a sentence, opposed to a random word, is more likely to be relevant to the context and retain the original label of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5itS2lJRmvV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Example of Random Insertion: hey man human beings how are valet you doing\n"
     ]
    }
   ],
   "source": [
    "from eda import random_insertion\n",
    "\n",
    "print(f\" Example of Random Insertion: {random_insertion('hey man how are you doing', n=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font color='red'> NB: Since we use `sentence.split()` to get the words, we end up with words that also contain punctuation, so we may not find a synonym for the word when in fact there is one.  \n",
    "For example, for the sentence `'Monitoring, reporting, and recordkeeping requirements. '` present in the training dataset, `sentence.split()` gives `['Monitoring,', 'reporting,' 'and', 'recordkeeping', 'requirements.']` and therefore no word will have any synonyms when in fact there are some! </font> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGOlMl3n1SyJ"
   },
   "source": [
    "### **4.2 Augment Your Model**\n",
    "\n",
    "Combine all the functions you have implemented in 4.1, you can come up with your own data augmentation pipeline with various p and n ;)\n",
    "\n",
    "Next step is to expand the training data you used in Task1, re-train your model in 1.4 on your augmented data, and re-evaluate its performance on both the given validation set as well as on your manually annotated 100 test datapoints. \n",
    "\n",
    "Discuss the improvements that your data augmentation brings to your model. ***Include some examples of old vs. new model predictions to demonstrate the improvements.***\n",
    "\n",
    "**Warning: In terms of data size and training time control, we stipulate that your augmented training data should not be larger than 100M.** (Currently the training data train.jsonl is about 25M.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "d0pTcUsYzsGZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original Sentence : hey man how are you doing\n",
      " SR Augmented Sentence : hey gentleman how are you doing\n",
      " RD Augmented Sentence : hey man are you\n",
      " RS Augmented Sentence : man hey how are you doing\n",
      " RI Augmented Sentence : hey man humankind how are you come doing\n"
     ]
    }
   ],
   "source": [
    "def aug(sent, n, p):\n",
    "    print(f\" Original Sentence : {sent}\")\n",
    "    print(f\" SR Augmented Sentence : {synonym_replacement(sent, n)}\")\n",
    "    print(f\" RD Augmented Sentence : {random_deletion(sent, p, n)}\")\n",
    "    print(f\" RS Augmented Sentence : {swap_word(sent)}\")\n",
    "    print(f\" RI Augmented Sentence : {random_insertion(sent, n)}\")\n",
    "    \n",
    "aug('hey man how are you doing', p=0.2, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-0U-CD323iY"
   },
   "source": [
    "- Augment training dataset and Re-train your model\n",
    "> Notes: you can decide on your own how much data you want to augment. But there are two pitfalls: i) by EDA, more augmentation means more noises, which not necessarily increases the performance; ii) more data means longer training time. Please balance your data scale and GPU time ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uExoah4O1ZCC"
   },
   "outputs": [],
   "source": [
    "# Data augmentation pipeline with various p and n on one sentence\n",
    "\n",
    "def aug_sent(sent):\n",
    "\n",
    "    # synonym_replacement\n",
    "    sent = synonym_replacement(sent, n=3)\n",
    "    \n",
    "    # random_deletion\n",
    "    sent = random_deletion(sent, p=0.4, max_deletion_n=6)\n",
    "    \n",
    "    # swap_word\n",
    "    sent = swap_word(sent)\n",
    "    \n",
    "    # random_insertion\n",
    "    sent = random_insertion(sent, n=3)\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "98176it [02:07, 770.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98176it [01:18, 1256.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9815it [00:07, 1370.47it/s]\n",
      "C:\\Users\\felic\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:52<00:00,  6.44it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:25<00:00, 23.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 1.047 | Validation Loss: 0.850\n",
      "Epoch 0 NLI Validation:\n",
      "Accuracy: 63.09% | F1: (67.53%, 62.29%, 57.79%) | Macro-F1: 62.54%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:44<00:00,  6.50it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:25<00:00, 23.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.934 | Validation Loss: 0.830\n",
      "Epoch 1 NLI Validation:\n",
      "Accuracy: 62.07% | F1: (68.04%, 58.97%, 54.80%) | Macro-F1: 60.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:49<00:00,  6.46it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:26<00:00, 23.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training Loss: 0.723 | Validation Loss: 0.926\n",
      "Epoch 2 NLI Validation:\n",
      "Accuracy: 62.95% | F1: (68.77%, 55.05%, 61.49%) | Macro-F1: 61.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6136/6136 [15:49<00:00,  6.46it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:25<00:00, 23.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training Loss: 0.404 | Validation Loss: 1.210\n",
      "Epoch 3 NLI Validation:\n",
      "Accuracy: 60.98% | F1: (67.47%, 51.03%, 60.31%) | Macro-F1: 59.60%\n"
     ]
    }
   ],
   "source": [
    "# Expand the training data from Task1 and re-train the model on this augmented data\n",
    "# Re-evaluate performance of the model on the given validation set\n",
    "\n",
    "    # 1. Set-up\n",
    "    \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "model.to(device)\n",
    "\n",
    "    # 2. Expand training data\n",
    "\n",
    "augm_training_data = []\n",
    "with jsonlines.open(\"nli_data/train.jsonl\", \"r\") as reader:\n",
    "    for augm in tqdm(reader.iter()):\n",
    "        augm[\"premise\"] = aug_sent(augm[\"premise\"])\n",
    "        augm[\"hypothesis\"] = aug_sent(augm[\"hypothesis\"])\n",
    "        augm_training_data.append(augm)\n",
    "\n",
    "with open(\"nli_data/train_augmented.jsonl\", \"w\") as writer:\n",
    "    for dict_ in augm_training_data:\n",
    "        jout = json.dumps(dict_) + '\\n'\n",
    "        writer.write(jout)\n",
    "\n",
    "    # 3. Get train_dataset and dev_dataset\n",
    "    \n",
    "train_dataset = NLIDataset(\"nli_data/train_augmented.jsonl\", tokenizer)\n",
    "dev_dataset = NLIDataset(\"nli_data/dev_in_domain.jsonl\", tokenizer)\n",
    "\n",
    "    # 4. Train and evaluate\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "warmup_percent = 0.3\n",
    "model_save_root = 'runs_augm/'\n",
    "learning_rate = 2e-5\n",
    "\n",
    "train(train_dataset, dev_dataset, model, device, batch_size, epochs,\n",
    "      learning_rate, warmup_percent, max_grad_norm, model_save_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building NLI Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 975.04it/s]\n",
      "Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 28.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.719 | Accuracy: 71.00%\n",
      "F1: (73.24%, 62.07%, 76.06%) | Macro-F1: 70.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate performance of the model on the manually annotated 100 test datapoints\n",
    "\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "warmup_percent = 0.3\n",
    "checkpoint = 'runs_augm/lr{}-warmup{}'.format(learning_rate, warmup_percent)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(checkpoint)\n",
    "model.to(device)\n",
    "\n",
    "annotation_dataset = NLIDataset(\"nli_data/27-284220_final.jsonl\", tokenizer)\n",
    "val_loss, acc, f1_ent, f1_neu, f1_con = evaluate(annotation_dataset, model, device, batch_size, no_labels=False, result_save_file=\"predictions/our-ann_augm-model\")\n",
    "macro_f1 = (f1_ent + f1_neu + f1_con)/3\n",
    "    \n",
    "print(f'Validation Loss: {val_loss:.3f} | Accuracy: {acc*100:.2f}%')\n",
    "print(f'F1: ({f1_ent*100:.2f}%, {f1_neu*100:.2f}%, {f1_con*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#6699FF; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#6699FF\"> Conclusion: </font> </b>\n",
    "<ul style=\"list-style-type:circle\"> \n",
    "<li> <b> The performance of the trained model on the augmented dataset is worse compared to the previous performance. Indeed, here the validation scores for epoch3 are 61.0% for the accuracy and 59.6% for the macro-F1 while they were previously 77.0% for the accuracy and 76.8% for the macro-F1. Similarly, the validation scores for our annotated dataset are 71.0% for accuracy and 70.5% for macro-F1 while they were previously 81.0% for accuracy and 79.6% for macro-F1. </b></li>\n",
    "<li> <b> A solution could be to better optimize the increase of the training dataset by changing again the parameters n and p of each function of the pipeline for example. </b></li>\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at some examples where the model trained on the augmented dataset does less well or does better:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of old vs. new model predictions with our annotation dataset\n",
    "\n",
    "    # Gather old and new results\n",
    "    \n",
    "comparison_augm = {\"premise\":[], \"hypothesis\":[], \"domain\":[], \"label\":[], \"pred_old\":[], \"pred_new\":[]}\n",
    "\n",
    "with jsonlines.open(\"predictions/our-ann\", \"r\") as reader:\n",
    "    for l in reader.iter():        \n",
    "        comparison_augm[\"premise\"].append(l[\"premise\"])\n",
    "        comparison_augm[\"hypothesis\"].append(l[\"hypothesis\"])\n",
    "        comparison_augm[\"domain\"].append(l[\"domain\"])\n",
    "        comparison_augm[\"label\"].append(l[\"label\"])\n",
    "        comparison_augm[\"pred_old\"].append(l[\"prediction\"])\n",
    "\n",
    "with jsonlines.open(\"predictions/our-ann_augm-model\", \"r\") as reader:\n",
    "    for l in reader.iter():  \n",
    "        comparison_augm[\"pred_new\"].append(l[\"prediction\"])\n",
    "\n",
    "        \n",
    "    # Keep only results where old or new predictions different from label\n",
    "\n",
    "old_true_new_false = {\"premise\":[], \"hypothesis\":[], \"domain\":[], \"label\":[], \"pred_old\":[], \"pred_new\":[]}\n",
    "old_false_new_true = {\"premise\":[], \"hypothesis\":[], \"domain\":[], \"label\":[], \"pred_old\":[], \"pred_new\":[]}\n",
    "\n",
    "for i in range(len(comparison_augm[\"label\"])):\n",
    "    \n",
    "    premise = comparison_augm[\"premise\"][i]\n",
    "    hypothesis = comparison_augm[\"hypothesis\"][i]\n",
    "    domain = comparison_augm[\"domain\"][i]\n",
    "    label = comparison_augm[\"label\"][i]\n",
    "    pred_old = comparison_augm[\"pred_old\"][i]\n",
    "    pred_new = comparison_augm[\"pred_new\"][i]\n",
    "    \n",
    "    if (label==pred_old) & (label!=pred_new):\n",
    "        old_true_new_false[\"premise\"].append(premise)\n",
    "        old_true_new_false[\"hypothesis\"].append(hypothesis)\n",
    "        old_true_new_false[\"domain\"].append(domain)\n",
    "        old_true_new_false[\"label\"].append(label)\n",
    "        old_true_new_false[\"pred_old\"].append(pred_old)\n",
    "        old_true_new_false[\"pred_new\"].append(pred_new)\n",
    "    \n",
    "    elif (label!=pred_old) & (label==pred_new):\n",
    "        old_false_new_true[\"premise\"].append(premise)\n",
    "        old_false_new_true[\"hypothesis\"].append(hypothesis)\n",
    "        old_false_new_true[\"domain\"].append(domain)\n",
    "        old_false_new_true[\"label\"].append(label)\n",
    "        old_false_new_true[\"pred_old\"].append(pred_old)\n",
    "        old_false_new_true[\"pred_new\"].append(pred_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_label is right but new_label is false:\n",
      " \n",
      " ..................\n",
      "Example0:  \n",
      " \n",
      " ---> premise: Alamo , the site of the Texas defeat by Santa Ana; hoosegow from juzgado `court'; dinero `money,' a Spanish corruption of the Latin denarius; macho , from the same root as machete : he who wields a machete must be skillful and powerful, hence the word has come to mean `virile' and its associated noun, machismo , `virility. \n",
      " ---> hypothesis: Alamo, the greatly revered site of the Texas defeat by Santa Ana. \n",
      "===> Label: neutral,  old label: neutral, new label: entailment\n",
      " \n",
      " ..................\n",
      "Example1:  \n",
      " \n",
      " ---> premise: Controllers at centers rely so heavily on transponder signals that they usually do not display primary radar returns on their radar scopes. \n",
      " ---> hypothesis:  Controllers at centers rely on sight the most.\n",
      "===> Label: contradiction,  old label: contradiction, new label: entailment\n",
      " \n",
      " ..................\n",
      "Example2:  \n",
      " \n",
      " ---> premise: Gregory uses stream in the way normal in England; Giles consistently refers to gens in the hills of central Australia, though glen is not current (outside place-names) in contemporary Australian English. \n",
      " ---> hypothesis: Glen is not contemporary Australian English right now.\n",
      "===> Label: entailment,  old label: entailment, new label: contradiction\n",
      " \n",
      " ..................\n",
      "Example3:  \n",
      " \n",
      " ---> premise: Then, too, one must examine the techniques used to group the many languages of the world. \n",
      " ---> hypothesis: I've examined countless techniques, but none of them could be used to group languages.\n",
      "===> Label: contradiction,  old label: contradiction, new label: neutral\n"
     ]
    }
   ],
   "source": [
    "# See examples where old_label is right but new_label is false\n",
    "\n",
    "print(\"old_label is right but new_label is false:\")\n",
    "for i in range(4):\n",
    "    print(\" \\n ..................\")\n",
    "    print(\"Example{}:  \\n \\n ---> premise: {} \\n ---> hypothesis: {}\".format(i, old_true_new_false[\"premise\"][i], old_true_new_false[\"hypothesis\"][i]))\n",
    "    print(\"===> Label: {},  old label: {}, new label: {}\".format(old_true_new_false[\"label\"][i], old_true_new_false[\"pred_old\"][i], old_true_new_false[\"pred_new\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_label is false but new_label is right:\n",
      " \n",
      " ..................\n",
      "Example0:  \n",
      " \n",
      " ---> premise: And, um, she read, I forgot about the nursery rhymes. \n",
      " ---> hyptohesis: She told \"Peter Pan\" so wonderfully, the kids didn't want to sleep.\n",
      "===> Label: neutral,  old label: contradiction, new label: neutral\n",
      " \n",
      " ..................\n",
      "Example1:  \n",
      " \n",
      " ---> premise: What a surprise to hear, first, an American commentator on a televised golf tourney describe a reverse-necked putter colloquially, and then to hear the Japanese broadcaster translate that description into a terse sentence or two ending with the expression  bassackawad putta.  \n",
      " ---> hyptohesis: The American commentator's description was incorrect, and the translation fixed it.\n",
      "===> Label: neutral,  old label: contradiction, new label: neutral\n",
      " \n",
      " ..................\n",
      "Example2:  \n",
      " \n",
      " ---> premise: And none of the information conveyed in the White House video teleconference, at least in the first hour, was being passed to the NMCC. \n",
      " ---> hyptohesis: The White House knew of the attacks immediately.\n",
      "===> Label: neutral,  old label: contradiction, new label: neutral\n",
      " \n",
      " ..................\n",
      "Example3:  \n",
      " \n",
      " ---> premise: His goal was to provide the elegant shopping experience of major European boutiques while satisfying the American desire for product diversity. \n",
      " ---> hyptohesis: He wanted to attract both European and Americans customers.\n",
      "===> Label: neutral,  old label: entailment, new label: neutral\n"
     ]
    }
   ],
   "source": [
    "# See examples where old_label is false and new_label is right\n",
    "\n",
    "print(\"old_label is false but new_label is right:\")\n",
    "for i in range(4):\n",
    "    print(\" \\n ..................\")\n",
    "    print(\"Example{}:  \\n \\n ---> premise: {} \\n ---> hyptohesis: {}\".format(i, old_false_new_true[\"premise\"][i], old_false_new_true[\"hypothesis\"][i]))\n",
    "    print(\"===> Label: {},  old label: {}, new label: {}\".format(old_false_new_true[\"label\"][i], old_false_new_true[\"pred_old\"][i], old_false_new_true[\"pred_new\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius: 5px; border: 3px dashed#6699FF; padding: 10px;\">\n",
    "\n",
    "<b> <font color=\"#6699FF\"> Conclusion: </font> </b>\n",
    "\n",
    "<b> The model trained on the augmented dataset seems to be more neutral, and seems to be more successful in determining when a label is \"neutral\". On the other hand, it seems to be more prone to misinterpretation. </b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3CIeN_kaOQl"
   },
   "source": [
    "### **5 Upload Your Notebook, Data and Models**\n",
    "\n",
    "Please **rename** your filled jupyter notebook as **your Sciper number** and upload it to your GitHub Classroom repository, **with all cells run and output results shown**.\n",
    "\n",
    "**Note:** We are **not** responsible for re-running the cells in your notebook.\n",
    "\n",
    "Please also submit all your processed (e.g., anotated and augmented) datasets, as well as all your trained models in Task 1 and Task 4, in your GitHub Classroom repository.\n",
    "\n",
    "The datasets and models that you need to submit include:\n",
    "\n",
    "**1. The best model checkpoint you trained in the Section 1.2 \"Start Training and Validation!\"**\n",
    "\n",
    "**2. The best model prediction results in the Section 1.2 \"Fine-Grained Validation\"**\n",
    "\n",
    "**3. Your annotated test dataset in the Section 3.2 \"Annotate Your 100 Datapoints with Partner(s)\"**\n",
    "\n",
    "**4. Your augmented training data and best model checkpoint in the Section 4.2 \"Augment Your Model\"**\n",
    "\n",
    "**Note:** You may need to use [GitHub LFS](https://edstem.org/eu/courses/379/discussion/27240) for submitting large files."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:nlp] *",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
